{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8353e714",
   "metadata": {},
   "source": [
    "# Implementing Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4286805",
   "metadata": {},
   "source": [
    "# Implementing Linear Regression\n",
    "\n",
    "### Linear Regression Notation\n",
    "\n",
    "**Single feature:**\n",
    "\n",
    "$\\hat{y} = f(x) = w_0 + w_1 x$\n",
    "\n",
    "**Multiple features:**\n",
    "\n",
    "$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n$\n",
    "\n",
    "**Matrix form:**\n",
    "\n",
    "$\\hat{y} = X W$\n",
    "\n",
    "where  \n",
    "\n",
    "$X = \n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\dots & x_{1n} \\\\\n",
    "1 & x_{21} & x_{22} & \\dots & x_{2n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{m1} & x_{m2} & \\dots & x_{mn}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m \\times (n+1)}$\n",
    "\n",
    "- The first column of 1's accounts for the bias term $w_0$.  \n",
    "- $W = \n",
    "\\begin{bmatrix} \n",
    "w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_n \n",
    "\\end{bmatrix} \\in \\mathbb{R}^{(n+1) \\times 1}$  \n",
    "\n",
    "$\\hat{y} \\in \\mathbb{R}^{m \\times 1}$\n",
    "\n",
    "---\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "The error function measures the distance between predicted and actual values:\n",
    "\n",
    "$E(X) = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2$\n",
    "\n",
    "- Squaring ensures negative differences do not cancel positive ones.  \n",
    "- Some literature uses absolute value instead of squaring.  \n",
    "- The scaling factor $1/n$ is optional for optimization purposes.\n",
    "\n",
    "**Cost function:**\n",
    "\n",
    "$J(W) = (\\hat{y} - y)^2$\n",
    "\n",
    "---\n",
    "\n",
    "### Vectorized form\n",
    "\n",
    "For any vector $v$:  \n",
    "\n",
    "$v^2 = v^\\top v$\n",
    "\n",
    "Example:  \n",
    "\n",
    "$v = \n",
    "\\begin{bmatrix} v_0 \\\\ v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}, \n",
    "\\quad\n",
    "v^\\top = \n",
    "\\begin{bmatrix} v_0 & v_1 & v_2 & \\dots & v_n \\end{bmatrix}, \n",
    "\\quad\n",
    "v^\\top v = v_0^2 + v_1^2 + \\dots + v_n^2\n",
    "$\n",
    "\n",
    "So for the prediction error:  \n",
    "\n",
    "$(\\hat{y} - y)^2 = (\\hat{y} - y)^\\top (\\hat{y} - y) = (X W - y)^\\top (X W - y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f1d80f",
   "metadata": {},
   "source": [
    "## Minimizing the Linear Regression Cost Function\n",
    "\n",
    "Our goal is to **minimize**:\n",
    "\n",
    "$\\operatorname*{argmin}_W \\; (XW - y)^\\top (XW - y)$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: First derivative\n",
    "\n",
    "- To minimize, we first compute the **gradient** (first derivative).  \n",
    "- Setting the gradient to zero gives the **critical points**, which are candidates for the minimum.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Closed-form solution\n",
    "\n",
    "The cost function:\n",
    "\n",
    "$J(W) = (XW - y)^\\top (XW - y)$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Expand using transpose properties\n",
    "\n",
    "- $(A - B)^\\top = A^\\top - B^\\top$\n",
    "\n",
    "Example:\n",
    "\n",
    "$A = \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix}, \\quad\n",
    "B = \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}$\n",
    "\n",
    "Then:\n",
    "\n",
    "$(A - B)^\\top = A^\\top - B^\\top = \\begin{bmatrix} a_1 - b_1 & a_2 - b_2 \\end{bmatrix}$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Apply to $J(W)$\n",
    "\n",
    "$J(W) = ((XW)^\\top - y^\\top)(XW - y)$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$J(W) = (XW)^\\top XW - (XW)^\\top y - y^\\top XW + y^\\top y$\n",
    "\n",
    "- Scalars satisfy $(XW)^\\top y = y^\\top XW$, so we can simplify:\n",
    "\n",
    "$J(W) = (XW)^\\top XW - 2 (XW)^\\top y + y^\\top y$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Transpose property for matrix products\n",
    "\n",
    "- For matrices $A$ and $B$:\n",
    "\n",
    "$(AB)^\\top = B^\\top A^\\top$\n",
    "\n",
    "- Example:\n",
    "Let\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}, \n",
    "\\quad\n",
    "B = \\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 1: Multiply \\(AB\\)**\n",
    "\n",
    "$$\n",
    "AB = \n",
    "\\begin{bmatrix} \n",
    "a_{11} & a_{12} \\\\ \n",
    "a_{21} & a_{22} \n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    "b_{11} & b_{12} \\\\ \n",
    "b_{21} & b_{22} \n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix} \n",
    "a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\\\ \n",
    "a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 2: Transpose \\((AB)^\\top\\)**\n",
    "\n",
    "$$\n",
    "(AB)^\\top = \n",
    "\\begin{bmatrix} \n",
    "a_{11}b_{11} + a_{12}b_{21} & a_{21}b_{11} + a_{22}b_{21} \\\\ \n",
    "a_{11}b_{12} + a_{12}b_{22} & a_{21}b_{12} + a_{22}b_{22} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 3: Compute \\(B^\\top A^\\top\\)**\n",
    "\n",
    "$$\n",
    "B^\\top = \\begin{bmatrix} b_{11} & b_{21} \\\\ b_{12} & b_{22} \\end{bmatrix}, \n",
    "\\quad\n",
    "A^\\top = \\begin{bmatrix} a_{11} & a_{21} \\\\ a_{12} & a_{22} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "B^\\top A^\\top = \n",
    "\\begin{bmatrix} b_{11} & b_{21} \\\\ b_{12} & b_{22} \\end{bmatrix} \n",
    "\\begin{bmatrix} a_{11} & a_{21} \\\\ a_{12} & a_{22} \\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix} \n",
    "b_{11}a_{11} + b_{21}a_{12} & b_{11}a_{21} + b_{21}a_{22} \\\\ \n",
    "b_{12}a_{11} + b_{22}a_{12} & b_{12}a_{21} + b_{22}a_{22} \n",
    "\\end{bmatrix}\n",
    "= (AB)^\\top\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208bc69e",
   "metadata": {},
   "source": [
    "## Derivative of the Linear Regression Cost Function\n",
    "\n",
    "We want to minimize the cost function:\n",
    "\n",
    "$$\n",
    "J(W) = (XW - Y)^\\top (XW - Y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: First term: $W^\\top X^\\top X W$\n",
    "\n",
    "Let \n",
    "\n",
    "$$\n",
    "A = X^\\top X\n",
    "$$\n",
    "\n",
    "so the first term is \n",
    "\n",
    "$$\n",
    "W^\\top A W\n",
    "$$\n",
    "\n",
    "Differential:\n",
    "\n",
    "$$\n",
    "d(W^\\top A W) = dW^\\top A W + W^\\top A dW\n",
    "$$\n",
    "\n",
    "- $W$ is $n \\times 1$, $dW$ is $n \\times 1$, $dW^\\top$ is $1 \\times n$, $AW$ is $n \\times 1$  \n",
    "- $dW^\\top A W$ is a scalar, and scalars equal their transpose:\n",
    "\n",
    "$$\n",
    "dW^\\top A W = (dW^\\top A W)^\\top = (AW)^\\top dW\n",
    "$$\n",
    "\n",
    "Similarly:\n",
    "\n",
    "$$\n",
    "W^\\top A dW = (A^\\top W)^\\top dW\n",
    "$$\n",
    "\n",
    "Combine:\n",
    "\n",
    "$$\n",
    "d(W^\\top A W) = (AW + A^\\top W)^\\top dW\n",
    "$$\n",
    "\n",
    "Since $A = X^\\top X$ is symmetric:\n",
    "\n",
    "$$\n",
    "d(W^\\top A W) = 2 (X^\\top X W)^\\top dW\n",
    "$$\n",
    "\n",
    "Gradient:\n",
    "\n",
    "$$\n",
    "\\nabla_W (W^\\top X^\\top X W) = 2 X^\\top X W\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Second term: $-2 Y^\\top X W$\n",
    "\n",
    "Differential:\n",
    "\n",
    "$$\n",
    "d(-2 Y^\\top X W) = -2 d(Y^\\top X W) = -2 (X^\\top Y)^\\top dW\n",
    "$$\n",
    "\n",
    "Gradient:\n",
    "\n",
    "$$\n",
    "\\nabla_W (-2 Y^\\top X W) = -2 X^\\top Y\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Third term: $Y^\\top Y$\n",
    "\n",
    "- Constant with respect to $W$, derivative = 0\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Combine all gradients\n",
    "\n",
    "$$\n",
    "\\nabla_W J(W) = 2 X^\\top X W - 2 X^\\top Y\n",
    "$$\n",
    "\n",
    "Set gradient to zero:\n",
    "\n",
    "$$\n",
    "2 X^\\top X W - 2 X^\\top Y = 0\n",
    "\\quad \\implies \\quad\n",
    "X^\\top X W = X^\\top Y\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Closed-form solution\n",
    "\n",
    "$$\n",
    "\\boxed{W = (X^\\top X)^{-1} X^\\top Y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703121b7",
   "metadata": {},
   "source": [
    "Let's try with the housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2be56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
