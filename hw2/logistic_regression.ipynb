{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5742fe0d",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Sigmoid function\n",
    "\n",
    "Squeezes the values from (-k, k) to (0, 1).  \n",
    "\n",
    "$$\n",
    "F(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "After being squeezed, we can make a **decision boundary** to determine which \"class\" the function belongs to.  \n",
    "\n",
    "Useful for **Classification ML tasks**.\n",
    "\n",
    "---\n",
    "\n",
    "## Linear equation\n",
    "\n",
    "We start with a linear equation \\(p(x)\\).  \n",
    "\n",
    "- \\(p(x)\\) is **unbounded**: domain \\((-\\infty, \\infty)\\)  \n",
    "- Use **logit transformation** to map to probability:  \n",
    "\n",
    "$$\n",
    "\\text{logit}(p(x)) = \\ln \\frac{p(x)}{1 - p(x)} = a + bx\n",
    "$$\n",
    "\n",
    "- Odds formula:  \n",
    "\n",
    "$$\n",
    "\\text{odds} = \\frac{p}{1-p}\n",
    "$$\n",
    "\n",
    "- Example:  \n",
    "  - \\(0.25 / (1-0.25) = 1/3\\) → 1 to 3  \n",
    "  - \\(0.8 / (1-0.8) = 4\\) → 4 to 1  \n",
    "\n",
    "Linear function:  \n",
    "\n",
    "$$\n",
    "f(x) = w_0 + w_1 x + \\dots + w_n x_n\n",
    "$$\n",
    "\n",
    "- \\(p = w_0 + w_1 x\\) could be negative or greater than 1 (**unbounded**).\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Linear function\n",
    "\n",
    "$$\n",
    "f(x)\n",
    "$$\n",
    "\n",
    "## 2. Logit (log-odds)\n",
    "\n",
    "$$\n",
    "\\text{logit}(f(x)) = \\ln \\frac{p(x)}{1 - p(x)} = f(x)\n",
    "$$\n",
    "\n",
    "## 3. Convert logit to probability\n",
    "\n",
    "$$\n",
    "\\ln \\frac{p(x)}{1 - p(x)} = f(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^{f(x)} = \\frac{p(x)}{1 - p(x)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^{f(x)} - e^{f(x)} p(x) = p(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(x) + e^{f(x)} p(x) = e^{f(x)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(x) (1 + e^{f(x)}) = e^{f(x)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{e^{f(x)}}{1 + e^{f(x}} )\n",
    "$$\n",
    "\n",
    "Divide numerator and denominator by \\(e^{f(x)}\\):  \n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{1 + e^{-f(x)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Likelihood\n",
    "\n",
    "Bernoulli: 0/1 probabilities  \n",
    "\n",
    "$$\n",
    "P(y|x) =\n",
    "\\begin{cases} \n",
    "p(x), & y = 1 \\\\\n",
    "1 - p(x), & y = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Can also be written as:  \n",
    "\n",
    "$$\n",
    "P(y|x) = p(x)^y (1 - p(x))^{1-y}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Log-likelihood\n",
    "\n",
    "$$\n",
    "\\log \\text{likelihood} = y \\ln p(x) + (1-y) \\ln (1-p(x))\n",
    "$$\n",
    "\n",
    "Loss over dataset:  \n",
    "\n",
    "$$\n",
    "l(w) = \\sum_{i=1}^n \\big[ y_i \\ln p(x_i) + (1-y_i) \\ln (1-p(x_i)) \\big]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Cross-entropy\n",
    "\n",
    "Minimize this function:  \n",
    "\n",
    "$$\n",
    "J(w) = - \\sum_{i=1}^n \\big[ y_i \\ln p(x_i) + (1-y_i) \\ln (1-p(x_i)) \\big]\n",
    "$$\n",
    "\n",
    "This is **cross-entropy loss**.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Gradient\n",
    "\n",
    "Remember:\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{1 + e^{-Xw}}\n",
    "$$\n",
    "\n",
    "Derivative steps:\n",
    "\n",
    "$$\n",
    "y \\ln p + (1-y) \\ln (1-p)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{y p'}{p} + \\frac{(1-y)(-p')}{1-p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{p' (y-p)}{p(1-p)}\n",
    "$$\n",
    "\n",
    "Substitute derivatives:\n",
    "(for the derivative of the bias i.e. with respect to b, there will be no -X on the denominator). That is the only difference.\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{1 + e^{-W^\\top X}}, \\quad p' = -X \\frac{e^{-W^\\top X}}{1 + e^{-W^\\top X}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "1-p = \\frac{e^{-W^\\top X}}{1 + e^{-W^\\top X}}\n",
    "$$\n",
    "\n",
    "Cancels out:  \n",
    "\n",
    "$$\n",
    "-X (y-p) = X (p-y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Gradient Descent\n",
    "\n",
    "Update weights during training:  \n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\, X (p-y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5244f225",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61e088fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "        self.history = []\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Maps a value between 0 and 1\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def loss_function(self, y, p):\n",
    "        \"\"\"\n",
    "        Cross-entropy loss function\n",
    "        \"\"\"\n",
    "        n = len(y)\n",
    "        eps = 1e-15\n",
    "        p = np.clip(p, eps, 1 - eps)\n",
    "        return -1/n * np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.n_iterations): \n",
    "            \n",
    "            p = self.sigmoid(X @ self.weights + self.bias)\n",
    "            loss = self.loss_function(y, p)\n",
    "            \n",
    "            # Gradients\n",
    "            dw = (1/n) * X.T @ (p - y)\n",
    "            db = (1/n) * np.sum(p - y)\n",
    "            \n",
    "            # Update the weights\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            self.history.append(loss)\n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Given input X, predict and return output y_pred\n",
    "        \"\"\"\n",
    "        p = self.sigmoid(X @ self.weights + self.bias)\n",
    "        return (p >= 0.5).astype(int)\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        MSE \n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X_test)\n",
    "        return np.mean((y_test - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0e344c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5ac0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "3     0   61        3.0              1        30.0     0.0                0   \n",
       "4     0   46        3.0              1        23.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
       "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
       "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
       "\n",
       "   TenYearCHD  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"framingham.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10061cda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['male', 'age', 'education', 'currentSmoker', 'cigsPerDay', 'BPMeds',\n",
       "       'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'sysBP',\n",
       "       'diaBP', 'BMI', 'heartRate', 'glucose', 'TenYearCHD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a141bab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['male', 'age', 'education', 'currentSmoker', 'cigsPerDay', 'BPMeds',\n",
    "       'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'sysBP',\n",
    "       'diaBP', 'BMI', 'heartRate', 'glucose']]\n",
    "y = df[[\"TenYearCHD\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63ac2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2805649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values.flatten()\n",
    "y_test = y_test.values.flatten() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f37d93c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51484c81",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14622641509433962"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = model.score(X_test, y_test)\n",
    "mse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daef0e3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.history)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
